@article{Arashloo2014,
author = {Arashloo, Shervin Rahimzadeh and Kittler, Josef},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Arashloo, Kittler - 2014 - Class-Specific Kernel Fusion of Multiple Descriptors for Face Verification Using Multiscale Binarised.pdf:pdf},
number = {12},
pages = {2100--2109},
title = {{Class-Specific Kernel Fusion of Multiple Descriptors for Face Verification Using Multiscale Binarised}},
volume = {9},
year = {2014}
}
@article{Arora2013,
abstract = {We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an \$n\$ node multilayer neural net that has degree at most \$n\^{}\{\backslash gamma\}\$ for some \$\backslash gamma <1\$ and each edge has a random edge weight in \$[-1,1]\$. Our algorithm learns \{$\backslash$em almost all\} networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights.},
archivePrefix = {arXiv},
arxivId = {1310.6343},
author = {Arora, Sanjeev and Bhaskara, Aditya and Ge, Rong and Ma, Tengyu},
eprint = {1310.6343},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Arora et al. - 2013 - Provable Bounds for Learning Some Deep Representations.pdf:pdf},
isbn = {9781634393973},
journal = {arXiv preprint arXiv:1310.6343},
pages = {18},
title = {{Provable Bounds for Learning Some Deep Representations}},
url = {http://arxiv.org/abs/1310.6343},
volume = {32},
year = {2013}
}
@article{Bengio2012,
abstract = {Learning algorithms related to artificial neural net- works and in particular for Deep Learning may seem to involve many bells and whistles, called hyper- parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back- propagated gradient and gradient-based optimiza- tion. It also discusses how to deal with the fact that more interesting results can be obtained when allow- ing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observedwith deeper architectures. 1},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8-26},
eprint = {1206.5533},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Bengio - 2012 - Practical recommendations for gradient-based training of deep architectures.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {437--478},
title = {{Practical recommendations for gradient-based training of deep architectures}},
volume = {7700 LECTU},
year = {2012}
}
@book{Bengio2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua},
booktitle = {Foundations and Trends® in Machine Learning},
doi = {10.1561/2200000006},
eprint = {0500581},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Bengio - 2009 - Learning Deep Architectures for AI.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
number = {1},
pages = {1--127},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@article{Bengio2009a,
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illus- trates gradually more concepts, and gradu- ally more complex ones. Here, we formal- ize such training strategies in the context of machine learning, and call them “curricu- lum learning”. In the context of recent re- search studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neu- ral networks), we explore curriculum learn- ing in various set-ups. The experiments show that significant improvements in generaliza- tion can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
doi = {10.1145/1553374.1553380},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Bengio et al. - 2009 - Curriculum learning.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th annual international conference on machine learning},
pages = {41--48},
pmid = {5414602},
title = {{Curriculum learning}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553380},
year = {2009}
}
@article{Best-rowden2013,
abstract = {As face recognition applications progress from con- strained sensing and cooperative subjects scenarios (e.g., driver’s license and passport photos) to unconstrained scenarios with uncooperative subjects (e.g., video surveillance), new challenges are encountered. These challenges are due to variations in ambient illumination, image resolution, background clutter, facial pose, expression, and occlusion. In forensic investigations where the goal is to identify a “person of interest,” often based on low quality face images and videos, we need to utilize whatever source of information is available about the person. This could include one or more video tracks, multiple still images captured by bystanders (using, for example, their mobile phones), 3D face models, and verbal descriptions of the subject provided by witnesses. These verbal descriptions can be used to generate a face sketch and provide ancillary information about the person of interest (e.g., gender, race, and age). While traditional face matching methods take single media (i.e., a still face image, video track, or face sketch) as input, our work considers using the entire gamut of media collection as a probe to generate a single candidate list for the person of interest. We show that the proposed approach boosts the likelihood of correctly identifying the person of interest through the use of different fusion schemes, 3D face models, and incorporation of quality measures for fusion and video frame selection. Index},
author = {Best-rowden, Lacey and Han, Hu and Otto, Charles and Klare, Brendan and Jain, Anil K},
doi = {10.1109/TIFS.2014.2359577},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Best-rowden et al. - 2013 - Unconstrained Face Recognition Identifying a Person of Interest from a Media Collection.pdf:pdf},
issn = {15566013},
pages = {1--13},
title = {{Unconstrained Face Recognition : Identifying a Person of Interest from a Media Collection}},
year = {2013}
}
@article{Cao2013,
abstract = {Face verification involves determining whether a pair of facial images belongs to the same or different subjects. This problem can prove to be quite challenging in many im- portant applications where labeled training data is scarce, e.g., family album photo organization software. Herein we propose a principled transfer learning approach for merg- ing plentiful source-domain data with limited samples from some target domain of interest to create a classifier that ide- ally performs nearly as well as if rich target-domain data were present. Based upon a surprisingly simple generative Bayesian model, our approach combines a KL-divergence- based regularizer/prior with a robust likelihood function leading to a scalable implementation via the EM algorithm. As justification for our design choices, we later use prin- ciples from convex analysis to recast our algorithm as an equivalent structured rank minimization problem leading to a number of interesting insights related to solution struc- ture and feature-transform invariance. These insights help to both explain the effectiveness of our algorithm as well as elucidate a wide variety of related Bayesian approaches. Experimental testing with challenging datasets validate the utility of the proposed algorithm},
author = {Cao, Xudong and Wipf, David and Wen, Fang and Duan, Genquan and Sun, Jian},
doi = {10.1109/ICCV.2013.398},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Cao et al. - 2013 - A Practical Transfer Learning Algorithm for Face Verification.pdf:pdf},
isbn = {978-1-4799-2840-8},
issn = {1550-5499},
journal = {2013 IEEE International Conference on Computer Vision},
pages = {3208--3215},
title = {{A Practical Transfer Learning Algorithm for Face Verification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751510},
year = {2013}
}
@article{Casia-webface,
author = {Casia-webface, Introduction and Academy, Chinese},
title = {{CASIA-WebFace DATABASE RELEASE AGREEMENT}}
}
@article{Chan2014,
abstract = {In this work, we propose a very simple deep learning network for image classification which comprises only the very basic data processing components: cascaded principal component analysis (PCA), binary hashing, and block-wise histograms. In the proposed architecture, PCA is employed to learn multistage filter banks. It is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus named as a PCA network (PCANet) and can be designed and learned extremely easily and efficiently. For comparison and better understanding, we also introduce and study two simple variations to the PCANet, namely the RandNet and LDANet. They share the same topology of PCANet but their cascaded filters are either selected randomly or learned from LDA. We have tested these basic networks extensively on many benchmark visual datasets for different tasks, such as LFW for face verification, MultiPIE, Extended Yale B, AR, FERET datasets for face recognition, as well as MNIST for hand-written digits recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state of the art features, either prefixed, highly hand-crafted or carefully learned (by DNNs). Even more surprisingly, it sets new records for many classification tasks in Extended Yale B, AR, FERET datasets, and MNIST variations. Additional experiments on other public datasets also demonstrate the potential of the PCANet serving as a simple but highly competitive baseline for texture classification and object recognition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.3606v1},
author = {Chan, Th and Jia, Kui and Gao, Shenghua and Lu, Jiwen and Zeng, Zinan and Ma, Yi},
eprint = {arXiv:1404.3606v1},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Chan et al. - 2014 - PCANet A Simple Deep Learning Baseline for Image Classification.pdf:pdf},
journal = {arXiv preprint},
pages = {1--15},
title = {{PCANet: A Simple Deep Learning Baseline for Image Classification?}},
url = {http://arxiv.org/abs/1404.3606},
year = {2014}
}
@article{Chen,
author = {Chen, Dong and Cao, Xudong and Wang, Liwei and Wen, Fang and Sun, Jian},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Chen et al. - Unknown - Bayesian Face Revisited A Joint Formulation.pdf:pdf},
number = {1},
title = {{Bayesian Face Revisited: A Joint Formulation}}
}
@article{Chena,
author = {Chen, Dong and Cao, Xudong and Wang, Liwei and Wen, Fang and Sun, Jian},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Chen et al. - Unknown - Supplemental material for “ Bayesian Face Revisited A Joint Formulation ” Efficient Computation for Block-wise.pdf:pdf},
pages = {1--5},
title = {{Supplemental material for “ Bayesian Face Revisited : A Joint Formulation ” Efficient Computation for Block-wise Matrix}}
}
@article{Chen2013,
abstract = {传统使用高维(100K)的特征对于人脸识别不太好。本文首先展示了高维特征(100K)的能力，识别率很好。其次使用一个投影方法(rotated sparse regression)，能将维数降低100倍，同时不牺牲准确度。},
author = {Chen, Dong and Cao, Xudong and Wen, Fang and Sun, Jian},
doi = {10.1109/CVPR.2013.389},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Chen et al. - 2013 - Blessing of dimensionality High-dimensional feature and its efficient compression for face verification.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
keywords = {Face Recognition,High-dimensional LBP,Rotated Sparse Regression},
pages = {3025--3032},
title = {{Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification}},
year = {2013}
}
@article{Chenb,
author = {Chen, Dong and Ren, Shaoqing and Wei, Yichen and Cao, Xudong and Sun, Jian},
doi = {10.1007/978-3-319-10599-4\_8},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Chen et al. - Unknown - Joint cascade face detection and alignment.pdf:pdf},
isbn = {978-3-319-10598-7},
title = {{Joint cascade face detection and alignment}}
}
@article{Coates2013,
abstract = {Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloud- like computing infrastructure and thousands of CPU cores. In this paper, we present tech- nical details and results from our own sys- tem based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) tech- nology: a cluster of GPU servers with Infini- band interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.},
author = {Coates, Adam and Huval, Brody and Wang, Tao and Wu, David and Ng, Andrew Y},
doi = {10.1177/0278364910371999},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Coates et al. - 2013 - Deep learning with COTS HPC systems.pdf:pdf},
issn = {0278-3649},
journal = {Proceedings of The 30th International Conference on Machine Learning},
pages = {1337--1345},
title = {{Deep learning with COTS HPC systems}},
year = {2013}
}
@article{Dean2012,
abstract = {Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm},
author = {Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark Z and Ranzato, Marc Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Dean et al. - 2012 - Large Scale Distributed Deep Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {NIPS 2012: Neural Information Processing Systems},
pages = {1--11},
title = {{Large Scale Distributed Deep Networks}},
year = {2012}
}
@article{Deng2013,
abstract = {We develop a Transform-Invariant PCA (TIPCA) technique which aims to accurately characterize the intrinsic structures of the human face that are invariant to the in-plane transformations of the training images. Specially, TIPCA alternately aligns the image ensemble and creates the optimal eigenspace with the objective to minimize the mean square error between the aligned images and their reconstructions. The learning from the FERET facial image ensemble of 1196 subjects validates the mutual promotion between image alignment and eigenspace representation, which eventually leads to the optimized coding and recognition performance that surpasses the manual alignment based approaches. Experimental results also suggest that state-of-the-art invariant descriptors, such as local binary pattern (LBP), histogram of oriented gradient (HOG), and gabor energy filter (GEF), and classification methods, such as sparse representation based classification (SRC) and support vector machine (SVM), can benefit from using the TIPCA-aligned faces, instead of the manually eye-aligned faces that are widely regarded as the ground-truth alignment. Favorable accuracies against the state-of-the-art results on face coding and face recognition are reported.},
author = {Deng, Weihong and Hu, Jiani and Lu, Jiwen and Guo, Jun},
doi = {AC37C0FF-FF8D-456D-9D9E-36CF42BFEB14},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Deng et al. - 2013 - Transform-Invariant PCA A Unified Approach to Fully Automatic Face Alignment, Representation, and Recognition.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {6},
pages = {1275--1284},
pmid = {24101334},
title = {{Transform-Invariant PCA: A Unified Approach to Fully Automatic Face Alignment, Representation, and Recognition.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24101334},
volume = {36},
year = {2013}
}
@article{Fan2014,
abstract = {Face representation is a crucial step of face recognition systems. An optimal face representation should be discriminative, robust, compact, and very easy-to-implement. While numerous hand-crafted and learning-based representations have been proposed, considerable room for improvement is still present. In this paper, we present a very easy-to-implement deep learning framework for face representation. Our method bases on a new structure of deep network (called Pyramid CNN). The proposed Pyramid CNN adopts a greedy-filter-and-down-sample operation, which enables the training procedure to be very fast and computation-efficient. In addition, the structure of Pyramid CNN can naturally incorporate feature sharing across multi-scale face representations, increasing the discriminative ability of resulting representation. Our basic network is capable of achieving high recognition accuracy (\$85.8\backslash\%\$ on LFW benchmark) with only 8 dimension representation. When extended to feature-sharing Pyramid CNN, our system achieves the state-of-the-art performance (\$97.3\backslash\%\$) on LFW benchmark. We also introduce a new benchmark of realistic face images on social network and validate our proposed representation has a good ability of generalization.},
archivePrefix = {arXiv},
arxivId = {1403.2802},
author = {Fan, Haoqiang and Cao, Zhimin and Jiang, Yuning and Yin, Qi and Doudou, Chinchilla},
eprint = {1403.2802},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Fan et al. - 2014 - Learning Deep Face Representation.pdf:pdf},
journal = {arXiv preprint arXiv:1403.2802},
pages = {1--10},
title = {{Learning Deep Face Representation}},
url = {http://arxiv.org/abs/1403.2802},
year = {2014}
}
@article{Forssen2007,
abstract = {This paper introduces an affine invariant shape descriptor for maximally stable extremal regions (MSER). Affine invariant feature descriptors are normally computed by sampling the original grey-scale image in an invariant frame defined from each detected feature, but we instead use only the shape of the detected MSER itself. This has the advantage that features can be reliably matched regardless of the appearance of the surroundings of the actual region. The descriptor is computed using the scale invariant feature transform (SIFT), with the resampled MSER binary mask as input. We also show that the original MSER detector can be modified to achieve better scale invariance by detecting MSERs in a scale pyramid. We make extensive comparisons of the proposed feature against a SIFT descriptor computed on grey-scale patches, and also explore the possibility of grouping the shape descriptors into pairs to incorporate more context. While the descriptor does not perform as well on planar scenes, we demonstrate various categories of full 3D scenes where it outperforms the SIFT descriptor computed on grey-scale patches. The shape descriptor is also shown to be more robust to changes in illumination. We show that a system can achieve the best performance under a range of imaging conditions by matching both the texture and shape descriptors.},
author = {Forss\'{e}n, Per Erik and Lowe, David G.},
doi = {10.1109/ICCV.2007.4409025},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Forss\'{e}n, Lowe - 2007 - Shape descriptors for maximally stable extremal regions.pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
title = {{Shape descriptors for maximally stable extremal regions}},
year = {2007}
}
@article{Girshick2014,
abstract = {Can a large convolutional neural network trained for whole-image classification on ImageNet be coaxed into detecting objects in PASCAL? We show that the answer is yes, and that the resulting system is simple, scalable, and boosts mean average precision, relative to the venerable deformable part model, by more than 40\% (achieving a final mAP of 48\% on VOC 2007). Our framework combines powerful computer vision techniques for generating bottom-up region proposals with recent advances in learning high-capacity convolutional neural networks. We call the resulting system R-CNN: Regions with CNN features. The same framework is also competitive with state-of-the-art semantic segmentation methods, demonstrating its flexibility. Beyond these results, we execute a battery of experiments that provide insight into what the network learns to represent, revealing a rich hierarchy of discriminative and often semantically meaningful features.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Berkeley, U C and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Cvpr'14},
pages = {2--9},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
url = {http://arxiv.org/abs/1311.2524},
year = {2014}
}
@misc{Guerra2009,
abstract = {The integration of usable and flexible analysis support in modelling environments is a key success factor in Model-Driven Development. In this paradigm, models are the core asset from which code is automatically generated, and thus ensuring model correctness is a fundamental quality control activity. For this purpose, a common approach is to transform the system models into formal semantic domains for verification. However, if the analysis results are not shown in a proper way to the end-user (e.g. in terms of the original language) they may become useless. In this paper we present a novel DSVL called BaVeL that facilitates the flexible annotation of verification results obtained in semantic domains to different formats, including the context of the original language. BaVeL is used in combination with a consistency framework, providing support for all steps in a verification process: acquisition of additional input data, transformation of the system models into semantic domains, verification, and flexible annotation of analysis results. The approach has been validated analytically by the cognitive dimensions framework, and empirically by its implementation and application to several DSVLs. Here we present a case study of a notation in the area of Digital Libraries, where the analysis is performed by transformations into Petri nets and a process algebra. © 2008 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Guerra, Esther and de Lara, Juan and Malizia, Alessio and D\'{\i}az, Paloma},
booktitle = {Information and Software Technology},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {0402594v3},
isbn = {0950-5849},
issn = {09505849},
keywords = {Back-annotation,Consistency,Domain-specific visual languages,Formal methods,Model transformation,Modelling environments},
number = {4},
pages = {769--784},
primaryClass = {arXiv:cond-mat},
title = {{Supporting user-oriented analysis for multi-view domain-specific visual languages}},
volume = {51},
year = {2009}
}
@article{Hadjis2015,
abstract = {We present Caffe con Troll (CcT), a fully compatible end-to-end version of the popular framework Caffe with rebuilt internals. We built CcT to examine the performance characteristics of training and deploying general-purpose convolutional neural networks across different hardware architectures. We find that, by employing standard batching optimizations for CPU training, we achieve a 4.5x throughput improvement over Caffe on popular networks like CaffeNet. Moreover, with these improvements, the end-to-end training time for CNNs is directly proportional to the FLOPS delivered by the CPU, which enables us to efficiently train hybrid CPU-GPU systems for CNNs.},
archivePrefix = {arXiv},
arxivId = {1504.04343},
author = {Hadjis, Stefan and Abuzaid, Firas and Zhang, Ce and R\'{e}, Christopher},
eprint = {1504.04343},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Hadjis et al. - 2015 - Caffe con Troll Shallow Ideas to Speed Up Deep Learning.pdf:pdf},
pages = {0--5},
title = {{Caffe con Troll: Shallow Ideas to Speed Up Deep Learning}},
url = {http://arxiv.org/abs/1504.04343},
year = {2015}
}
@article{Hadsell2006,
abstract = { Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.},
author = {Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
doi = {10.1109/CVPR.2006.100},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Hadsell, Chopra, LeCun - 2006 - Dimensionality reduction by learning an invariant mapping.pdf:pdf},
isbn = {0769525970},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1735--1742},
title = {{Dimensionality reduction by learning an invariant mapping}},
volume = {2},
year = {2006}
}
@article{Hassner,
author = {Hassner, Tal and Enbar, Roee},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Hassner, Enbar - Unknown - Effective Face Frontalization in Unconstrained Images.pdf:pdf},
title = {{Effective Face Frontalization in Unconstrained Images}}
}
@article{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
doi = {10.1007/b94608},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Hastie, Tibshirani, Friedman - 2009 - The Elements of Statistical Learning.pdf:pdf},
isbn = {9780387848570},
issn = {03436993},
journal = {Elements},
pages = {337--387},
pmid = {15512507},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/10.1007/b94608},
volume = {1},
year = {2009}
}
@article{Howard2013,
abstract = {We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techniques include adding more image transformations to the training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55\% using no external data which is over a 20\% relative improvement on the previous year’s winner.},
archivePrefix = {arXiv},
arxivId = {1312.5402},
author = {Howard, Ag},
eprint = {1312.5402},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Howard - 2013 - Some Improvements on Deep Convolutional Neural Network Based Image Classification.pdf:pdf},
journal = {arXiv preprint arXiv:1312.5402},
title = {{Some Improvements on Deep Convolutional Neural Network Based Image Classification}},
url = {http://arxiv.org/abs/1312.5402},
year = {2013}
}
@article{Hu,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.02351v1},
author = {Hu, Guosheng and Yang, Yongxin and Yi, Dong and Kittler, Josef and Christmas, William and Li, Stan Z and Hospedales, Timothy},
eprint = {arXiv:1504.02351v1},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Hu et al. - Unknown - When Face Recognition Meets with Deep Learning an Evaluation of Convolutional Neural Networks for Face Recognitio.pdf:pdf},
title = {{When Face Recognition Meets with Deep Learning : an Evaluation of Convolutional Neural Networks for Face Recognition}}
}
@article{Huang2012,
abstract = {Unsupervised joint alignment of images has been demonstrated to improve per- formance on recognition tasks such as face verification. Such alignment reduces undesired variability due to factors such as pose, while only requiring weak su- pervision in the form of poorly aligned examples. However, prior work on unsu- pervised alignment of complex, real-world images has required the careful selec- tion of feature representation based on hand-crafted image descriptors, in order to achieve an appropriate, smooth optimization landscape. In this paper, we instead propose a novel combination of unsupervised joint alignment with unsupervised feature learning. Specifically, we incorporate deep learning into the congealing alignment framework. Through deep learning, we obtain features that can repre- sent the image at differing resolutions based on network depth, and that are tuned to the statistics of the specific data being aligned. In addition, we modify the learning algorithm for the restricted Boltzmann machine by incorporating a group sparsity penalty, leading to a topographic organization of the learned filters and improving subsequent alignment results. We apply our method to the Labeled Faces in the Wild database (LFW). Using the aligned images produced by our proposed unsupervised algorithm, we achieve higher accuracy in face verification compared to prior work in both unsupervised and supervised alignment. We also match the accuracy for the best available commercial method.},
author = {Huang, Gary B. and Mattar, Marwan a. and Lee, Honglak and Learned-Miller, Erik},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Huang et al. - 2012 - Learning to Align from Scratch.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Proc. Neural Information Processing Systems},
pages = {1--9},
title = {{Learning to Align from Scratch}},
year = {2012}
}
@article{Huang2014,
author = {Huang, Gary B and Learned-miller, Erik},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Huang, Learned-miller - 2014 - Labeled Faces in the Wild Updates and New Reporting Procedures.pdf:pdf},
pages = {1--5},
title = {{Labeled Faces in the Wild: Updates and New Reporting Procedures}},
year = {2014}
}
@misc{Huang,
author = {Huang, Gary B and Learned-miller, Erik},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Huang, Learned-miller - Unknown - Unsupervised Joint Alignment of Complex Images.pdf.pdf:pdf},
title = {{Unsupervised Joint Alignment of Complex Images.pdf}}
}
@article{Huang2007,
abstract = {Face recognition has benefitted greatly from the many databases that have been produced to study it. Most of these databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, expression, background, camera quality, occlusion, age, and gender. While there are many applications for face recognition technol- ogy in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database is provided as an aid in studying the latter, unconstrained, face recognition problem. The database represents an initial attempt to provide a set of labeled face photographs spanning the range of conditions typically encountered by people in their everyday lives. The database exhibits natural variability in pose, lighting, focus, resolution, facial expression, age, gender, race, accessories, make-up, occlusions, background, and photographic quality. Despite this variability, the images in the database are presented in a simple and consistent format for maximum ease of use. In addition to describing the details of the database and its acquisition, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible.},
author = {Huang, Gb and Ramesh, Manu and Berg, Tamara and Learned-Miller, Erik},
doi = {10.1.1.122.8268},
journal = {University of Massachusetts Amherst Technical Report 07},
number = {07-49},
pages = {1--11},
title = {{Labeled faces in the wild: A database for studying face recognition in unconstrained environments}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.122.8268\&amp;rep=rep1\&amp;type=pdf$\backslash$nhttps://www.cs.umass.edu/~elm/papers/lfw.pdf},
volume = {49},
year = {2007}
}
@article{Kazemi2014,
abstract = {This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face’s landmark positions directly from a sparse subse ...},
author = {Kazemi, Vahid and Josephine, S},
doi = {10.1109/CVPR.2014.241},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Kazemi, Josephine - 2014 - One Millisecond Face Alignment with an Ensemble of Regression Trees.pdf:pdf},
isbn = {978-1-4799-5118-5},
journal = {Computer Vision and Pattern Recognition (CVPR), 2014},
title = {{One Millisecond Face Alignment with an Ensemble of Regression Trees}},
url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2:713097},
year = {2014}
}
@article{Klein,
author = {Klein, Benjamin and Lev, Guy and Sadeh, Gil and Wolf, Lior},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Klein et al. - Unknown - Associating Neural Word Embeddings with Deep Image Representations using Fisher Vectors.pdf:pdf},
title = {{Associating Neural Word Embeddings with Deep Image Representations using Fisher Vectors}}
}
@article{Le2012,
abstract = {We consider the problem of building high- level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 bil- lion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a clus- ter with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental re- sults reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bod- ies. Starting with these learned features, we trained our network to obtain 15.8\% accu- racy in recognizing 20,000 object categories from ImageNet, a leap of 70\% relative im- provement over the previous state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1112.6209v3},
author = {Le, Quoc V. and Ranzato, Marc Aurelio and Devin, Matthieu and Corrado, Greg S. and Ng, Andrew Y.},
doi = {10.1109/MSP.2011.940881},
eprint = {1112.6209v3},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Le et al. - 2012 - Building High-level Features Using Large Scale Unsupervised Learning.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {10535888},
journal = {Arxiv},
number = {4},
pages = {61--76},
title = {{Building High-level Features Using Large Scale Unsupervised Learning}},
url = {http://arxiv.org/pdf/1112.6209v5.pdf},
volume = {28},
year = {2012}
}
@article{LeCun2015,
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
title = {{Deep learning}},
url = {http://www.nature.com/doifinder/10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Lepetit2004,
author = {Lepetit, V. and Pilet, J. and Fua, P.},
doi = {10.1109/CVPR.2004.1315170},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Lepetit, Pilet, Fua - 2004 - Point matching as a classification problem for fast and robust object pose estimation.pdf:pdf},
isbn = {0-7695-2158-4},
journal = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {244--250},
title = {{Point matching as a classification problem for fast and robust object pose estimation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1315170},
volume = {2},
year = {2004}
}
@article{Lev,
author = {Lev, Guy and Klein, Benjamin and Wolf, Lior},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Lev, Klein, Wolf - Unknown - In Defense of Word Embedding for Generic Text Representation.pdf:pdf},
title = {{In Defense of Word Embedding for Generic Text Representation}}
}
@article{Li,
author = {Li, Haoxiang and Hua, Gang},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Li, Hua - Unknown - Hierarchical-PEP Model for Real-world Face Recognition.pdf:pdf},
title = {{Hierarchical-PEP Model for Real-world Face Recognition}}
}
@article{Lienhart2003,
abstract = { This paper presents a novel tree classifier for complex object detection tasks together with a general framework for real-time object tracking in videos using the novel tree classifier. A boosted training algorithm with a clustering-and-splitting step is employed to construct branches in the nodes recursively, if and only if it improves the discriminative power compared to a single monolithic node classifier and has a lower computational complexity. A mouth tracking system that integrates the tree classifier under the proposed framework is built and tested on XM2FDB database. Experimental results show that the detection accuracy is equal or better than a single or multiple cascade classifier, while being computational less demanding.},
author = {Lienhart, R. and Liang, Luhong Liang Luhong and Kuranov, a.},
doi = {10.1109/ICME.2003.1221607},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Lienhart, Liang, Kuranov - 2003 - A detector tree of boosted classifiers for real-time object detection and tracking.pdf:pdf},
isbn = {0-7803-7965-9},
issn = {1945788X},
journal = {2003 International Conference on Multimedia and Expo. ICME '03. Proceedings (Cat. No.03TH8698)},
number = {c},
pages = {1--4},
title = {{A detector tree of boosted classifiers for real-time object detection and tracking}},
volume = {2},
year = {2003}
}
@article{Lienhart2002,
abstract = { Recently Viola et al. [2001] have introduced a rapid object detection. scheme based on a boosted cascade of simple feature classifiers. In this paper we introduce a novel set of rotated Haar-like features. These novel features significantly enrich the simple features of Viola et al. and can also be calculated efficiently. With these new rotated features our sample face detector shows off on average a 10\% lower false alarm rate at a given hit rate. We also present a novel post optimization procedure for a given boosted cascade improving on average the false alarm rate further by 12.5\%.},
archivePrefix = {arXiv},
arxivId = {hep-th/9209032v1},
author = {Lienhart, R. and Maydt, J.},
doi = {10.1109/ICIP.2002.1038171},
eprint = {9209032v1},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Lienhart, Maydt - 2002 - An extended set of Haar-like features for rapid object detection.pdf:pdf},
isbn = {0-7803-7622-6},
issn = {1522-4880},
journal = {Proceedings. International Conference on Image Processing},
pages = {0--3},
pmid = {17192338},
primaryClass = {hep-th},
title = {{An extended set of Haar-like features for rapid object detection}},
volume = {1},
year = {2002}
}
@article{Lin2013,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
eprint = {1312.4400},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Lin, Chen, Yan - 2013 - Network In Network.pdf:pdf},
journal = {arXiv preprint},
pages = {10},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2013}
}
@article{Lu2014,
abstract = {Face verification remains a challenging problem in very complex conditions with large variations such as pose, illumination, expression, and occlusions. This problem is exacerbated when we rely unrealistically on a single training data source, which is often insufficient to cover the intrinsically complex face variations. This paper proposes a principled multi-task learning approach based on Discriminative Gaussian Process Latent Variable Model, named GaussianFace, to enrich the diversity of training data. In comparison to existing methods, our model exploits additional data from multiple source-domains to improve the generalization performance of face verification in an unknown target-domain. Importantly, our model can adapt automatically to complex data distributions, and therefore can well capture complex face variations inherent in multiple sources. Extensive experiments demonstrate the effectiveness of the proposed model in learning from diverse data sources and generalize to unseen domain. Specifically, the accuracy of our algorithm achieves an impressive accuracy rate of 98.52\% on the well-known and challenging Labeled Faces in the Wild (LFW) benchmark. For the first time, the human-level performance in face verification (97.53\%) on LFW is surpassed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.3840v1},
author = {Lu, Chaochao and Tang, X},
eprint = {arXiv:1404.3840v1},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Lu, Tang - 2014 - Surpassing Human-Level Face Verification Performance on LFW with GaussianFace.pdf:pdf},
journal = {arXiv preprint arXiv:1404.3840},
title = {{Surpassing Human-Level Face Verification Performance on LFW with GaussianFace}},
url = {http://arxiv.org/abs/1404.3840},
year = {2014}
}
@article{Mcfee2011,
author = {Mcfee, Brian and Lanckriet, Gert},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Mcfee, Lanckriet - 2011 - Learning Multi-modal Similarity.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {metric learning,multiple kernel learning,similarity},
pages = {491--523},
title = {{Learning Multi-modal Similarity}},
volume = {12},
year = {2011}
}
@article{Mnih2014,
abstract = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
archivePrefix = {arXiv},
arxivId = {1406.6247},
author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
eprint = {1406.6247},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Mnih et al. - 2014 - Recurrent Models of Visual Attention.pdf:pdf},
pages = {1--12},
title = {{Recurrent Models of Visual Attention}},
url = {http://arxiv.org/abs/1406.6247},
year = {2014}
}
@article{Ngiam2011,
abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned ifmultiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evalu- ate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our mod- els are validated on the CUAVE and AVLet- ters datasets on audio-visual speech classifi- cation, demonstrating best published visual speech classification on AVLetters and effec- tive shared representation learning.},
author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Ngiam et al. - 2011 - Multimodal Deep Learning.pdf:pdf},
isbn = {9781450306195},
journal = {Proceedings of The 28th International Conference on Machine Learning (ICML)},
pages = {689--696},
title = {{Multimodal Deep Learning}},
year = {2011}
}
@article{Ren2014,
abstract = {This paper presents a highly efficient, very accurate regression approach for face alignment. Our approach has two novel components: a set of local binary features, and a locality principle for learning those features. The locality principle guides us to learn a set of highly discriminative local binary features for each facial landmark independently. The obtained local binary features are used to jointly learn a linear regression for the final output. Our approach achieves the state-of-the-art results when tested on the current most challenging benchmarks. Furthermore, because extracting and regressing local binary features is computationally very cheap, our system is much faster than previous methods. It achieves over 3,000 fps on a desktop or 300 fps on a mobile phone for locating a few dozens of landmarks.},
author = {Ren, Shaoqing and Cao, Xudong and Wei, Yichen and Sun, Jian},
doi = {10.1109/CVPR.2014.218},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Ren et al. - 2014 - Face Alignment at 3000 FPS via Regressing Local Binary Features.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Cvpr},
number = {1},
pages = {1--8},
title = {{Face Alignment at 3000 FPS via Regressing Local Binary Features}},
volume = {1},
year = {2014}
}
@article{Rizvi1998,
abstract = {Two critical performance characterizations of biometric algorithms, including face recognition, are identi cation and veri cation. In face recognition, FERET is the de facto standard evaluation methodology. Identi cation performance of face recog- nition algorithms on the FERET tests has been previously reported. In this paper we report on veri cation performance obtained from the Sep96 FERET test. Re- sults are presented for images taken on the same day, for images taken on di erent days, for images taken at least one year apart, and for images taken under di erent lighting conditions},
author = {Rizvi, Syed a and Rizvi, Syed a and Phillips, P Jonathon and Phillips, P Jonathon and Moon, Hyeonjoon and Moon, Hyeonjoon},
doi = {10.1109/AFGR.1998.670924},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Rizvi et al. - 1998 - The FERET Verification Testing Protocol for Face Recognition Algorithms.pdf:pdf},
isbn = {0-8186-8344-9},
journal = {Computer Engineering},
keywords = {algorithm evaluation,face recognition,feret,veri cation},
number = {October},
pages = {1--16},
title = {{The FERET Verification Testing Protocol for Face Recognition Algorithms}},
year = {1998}
}
@article{Saxe2011,
author = {Saxe, A and Koh, Pw and Chen, Zhenghao},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Saxe, Koh, Chen - 2011 - On random weights and unsupervised feature learning.pdf:pdf},
journal = {Proceedings of the \ldots},
pages = {1--9},
title = {{On random weights and unsupervised feature learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper\_files/ICML2011Saxe\_551.pdf},
year = {2011}
}
@article{Schmidhuber2014,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning $\backslash$\& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7828v1},
author = {Schmidhuber, J\"{u}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {arXiv:1404.7828v1},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Schmidhuber - 2014 - Deep Learning in Neural Networks An Overview.pdf:pdf},
isbn = {0893-6080},
issn = {0893-6080},
journal = {arXiv preprint arXiv:1404.7828},
pages = {1--66},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://arxiv.org/abs/1404.7828},
year = {2014}
}
@article{Schroff,
author = {Schroff, Florian and Philbin, James},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Schroff, Philbin - Unknown - FaceNet A Unified Embedding for Face Recognition and Clustering.pdf:pdf},
title = {{FaceNet : A Unified Embedding for Face Recognition and Clustering}}
}
@article{Silberpfennig,
author = {Silberpfennig, Adi and Wolf, Lior and Dershowitz, Nachum and Aviv, Tel},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Silberpfennig et al. - Unknown - Improving OCR for an Under-Resourced Script Using Unsupervised Word-Spotting.pdf:pdf},
title = {{Improving OCR for an Under-Resourced Script Using Unsupervised Word-Spotting}}
}
@article{Simonyan2013,
abstract = {Several recent papers on automatic face verification have significantly raised the per- formance bar by developing novel, specialised representations that outperform standard features such as SIFT for this problem. This paper makes two contributions: first, and somewhat surprisingly, we show that Fisher vectors on densely sampled SIFT features, i.e. an off-the-shelf object recognition representation, are capable of achieving state-of-the-art face verification performance on the challenging “Labeled Faces in the Wild” benchmark; second, since Fisher vectors are very high dimensional, we show that a compact descriptor can be learnt from them using discriminative metric learning. This compact descriptor has a better recognition accuracy and is very well suited to large scale identification tasks.},
author = {Simonyan, Karen and Parkhi, Omkar and Vedaldi, Andrea and Zisserman, Andrew},
doi = {10.5244/C.27.8},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Simonyan et al. - 2013 - Fisher Vector Faces in the Wild.pdf:pdf},
isbn = {1-901725-49-9},
journal = {Procedings of the British Machine Vision Conference 2013},
pages = {8.1--8.11},
title = {{Fisher Vector Faces in the Wild}},
url = {http://www.bmva.org/bmvc/2013/Papers/paper0008/index.html},
year = {2013}
}
@article{Simonyan2013a,
abstract = {Several recent papers on automatic face verification have significantly raised the per- formance bar by developing novel, specialised representations that outperform standard features such as SIFT for this problem. This paper makes two contributions: first, and somewhat surprisingly, we show that Fisher vectors on densely sampled SIFT features, i.e. an off-the-shelf object recognition representation, are capable of achieving state-of-the-art face verification performance on the challenging “Labeled Faces in the Wild” benchmark; second, since Fisher vectors are very high dimensional, we show that a compact descriptor can be learnt from them using discriminative metric learning. This compact descriptor has a better recognition accuracy and is very well suited to large scale identification tasks.},
author = {Simonyan, Karen and Parkhi, Omkar and Vedaldi, Andrea and Zisserman, Andrew},
doi = {10.5244/C.27.8},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Simonyan et al. - 2013 - Fisher Vector Faces in the Wild(2).pdf:pdf},
isbn = {1-901725-49-9},
journal = {Procedings of the British Machine Vision Conference 2013},
pages = {8.1--8.11},
title = {{Fisher Vector Faces in the Wild}},
url = {http://www.bmva.org/bmvc/2013/Papers/paper0008/index.html},
year = {2013}
}
@article{Simonyan2015,
archivePrefix = {arXiv},
arxivId = {1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556v6},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Simonyan, Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recoginition.pdf:pdf},
journal = {Intl. Conf. on Learning Representations (ICLR)},
pages = {1--14},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recoginition}},
year = {2015}
}
@article{Snoek2012,
abstract = {超パラメータのベイズ最適化},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Rp},
eprint = {1206.2944},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Snoek, Larochelle, Adams - 2012 - Practical Bayesian Optimization of Machine Learning Algorithms.pdf:pdf},
isbn = {9781627480031},
journal = {Nips},
pages = {1--9},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms.}},
url = {https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf},
year = {2012}
}
@article{Srivastava2012,
abstract = {We propose a Deep Boltzmann Machine for learning a generative model of multimodal data. We show how to use the model to extract a meaningful representation of multimodal data. We find that the learned representation is useful for classification and information retreival tasks, and hence conforms to some notion of semantic similarity. The model defines a probability density over the space of multimodal inputs. By sampling from the conditional distributions over each data modality, it possible to create the representation even when some data modalities are missing. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that our model can significantly outperform SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves significant gains.},
author = {Srivastava, Nitish and Salakhutdinov, Ruslan},
doi = {10.1109/CVPR.2013.49},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Srivastava, Salakhutdinov - 2012 - Multimodal Learning with Deep Boltzmann Machines.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10495258},
journal = {Nips},
pages = {2231--2239},
title = {{Multimodal Learning with Deep Boltzmann Machines.}},
year = {2012}
}
@article{Sun,
abstract = {The state-of-the-art of face recognition has been signifi-cantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net [10] and GoogLeNet [16] to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53\% LFW face verification accuracy and 96.0\% LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.00873v1},
author = {Sun, Yi and Liang, Ding and Wang, Xiaogang and Tang, Xiaoou},
eprint = {arXiv:1502.00873v1},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Sun et al. - Unknown - DeepID3 Face Recognition with Very Deep Neural Networks.pdf:pdf},
pages = {2--6},
title = {{DeepID3: Face Recognition with Very Deep Neural Networks}}
}
@article{Suna,
abstract = {The state-of-the-art of face recognition has been signifi-cantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net [10] and GoogLeNet [16] to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53\% LFW face verification accuracy and 96.0\% LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.00873v1},
author = {Sun, Yi and Liang, Ding and Wang, Xiaogang and Tang, Xiaoou},
eprint = {arXiv:1502.00873v1},
pages = {2--6},
title = {{DeepID3: Face Recognition with Very Deep Neural Networks}}
}
@article{Sun2014,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15\% face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67\%.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Sun, Wang, Tang - 2014 - Deep Learning Face Representation by Joint Identification-Verification.pdf:pdf},
isbn = {978-1-4799-5118-5},
pages = {1--9},
title = {{Deep Learning Face Representation by Joint Identification-Verification}},
url = {http://arxiv.org/abs/1406.4773},
year = {2014}
}
@article{Sun2014a,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15\% face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67\%.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
isbn = {978-1-4799-5118-5},
pages = {1--9},
title = {{Deep Learning Face Representation by Joint Identification-Verification}},
url = {http://arxiv.org/abs/1406.4773},
year = {2014}
}
@article{Sun2014b,
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
doi = {10.1109/CVPR.2014.244},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Sun, Wang, Tang - 2014 - Deep learning face representation from predicting 10,000 classes.pdf:pdf},
journal = {The IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Deep learning face representation from predicting 10,000 classes}},
url = {http://www.cv-foundation.org/openaccess/content\_cvpr\_2014/html/Sun\_Deep\_Learning\_Face\_2014\_CVPR\_paper.html},
year = {2014}
}
@article{Szegedy2014,
archivePrefix = {arXiv},
arxivId = {1409.4842v1},
author = {Szegedy, Christian and Reed, Scott and Sermanet, Pierre and Vanhoucke, Vincent and Rabinovich, Andrew},
eprint = {1409.4842v1},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Szegedy et al. - 2014 - Going deeper with convolutions.pdf:pdf},
pages = {1--12},
title = {{Going deeper with convolutions}},
year = {2014}
}
@article{Taigman2014,
abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect => align => represent => classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4,000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35\% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27\%, closely approaching human-level performance.},
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
doi = {10.1109/CVPR.2014.220},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Taigman et al. - 2014 - DeepFace Closing the Gap to Human-Level Performance in Face Verification.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {8},
title = {{DeepFace: Closing the Gap to Human-Level Performance in Face Verification}},
url = {http://www.cs.tau.ac.il/~wolf/papers/deepface\_11\_01\_2013.pdf},
year = {2014}
}
@article{Taigman2014a,
abstract = {Scaling machine learning methods to massive datasets has attracted considerable attention in recent years, thanks to easy access to ubiquitous sensing and data from the web. Face recognition is a task of great practical interest for which (i) very large labeled datasets exist, containing billions of images; (ii) the number of classes can reach tens of millions or more; and (iii) complex features are necessary in order to encode subtle differences between subjects, while maintaining invariance to factors such as pose, illumination, and aging. We present an elaborate pipeline that consists of a crucial network compression step followed by a new bootstrapping scheme for selecting a challenging subset of the dataset for efficient training of a higher capacity network. By using this approach, we are able to greatly improve face recognition accuracy on the widely used LFW benchmark. Moreover, as performance on supervised face verification (1:1) benchmarks saturates, we propose to shift the attention of the research community to the unsupervised Probe-Gallery (1:N) identification benchmarks. On this task, we bridge between the literature and the industry, for the first time, by directly comparing with the state of the art Commercially-Off-The-Shelf system and show a sizable leap in performance. Lastly, we demonstrate an intriguing trade-off between the number of training samples and the optimal size of the network.},
archivePrefix = {arXiv},
arxivId = {1406.5266},
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
eprint = {1406.5266},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Taigman et al. - 2014 - Web-Scale Training for Face Identification.pdf:pdf},
isbn = {9781467369640},
title = {{Web-Scale Training for Face Identification}},
url = {http://arxiv.org/abs/1406.5266.pdf},
year = {2014}
}
@article{Taigman2014b,
abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect => align => represent => classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4,000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35\% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27\%, closely approaching human-level performance.},
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
doi = {10.1109/CVPR.2014.220},
isbn = {9781479951178},
issn = {10636919},
journal = {Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {8},
title = {{DeepFace: Closing the Gap to Human-Level Performance in Face Verification}},
url = {http://www.cs.tau.ac.il/~wolf/papers/deepface\_11\_01\_2013.pdf},
year = {2014}
}
@article{Wang,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.1265v1},
author = {Wang, Xiaogang and Tang, Xiaoou},
eprint = {arXiv:1412.1265v1},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Wang, Tang - Unknown - Deeply learned face representations are sparse, selective, and robust.pdf:pdf},
title = {{Deeply learned face representations are sparse, selective, and robust}}
}
@article{Weinshall1989,
annote = {10.1038/341737a0},
author = {Weinshall, Daphna},
journal = {Nature},
month = oct,
number = {6244},
pages = {737--739},
title = {{Perception of multiple transparent planes in stereo vision}},
url = {http://dx.doi.org/10.1038/341737a0},
volume = {341},
year = {1989}
}
@article{Wright2009,
abstract = {We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by C1-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.},
author = {Wright, J. and a.Y. Yang},
doi = {10.1109/TPAMI.2008.79},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Wright, Yang - 2009 - Robust face recognition via sparse representation.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {Pattern Analysis and \ldots},
number = {2},
pages = {210--227},
pmid = {19110489},
title = {{Robust face recognition via sparse representation}},
url = {http://ieeexplore.ieee.org/ielx5/34/4731221/04483511.pdf?tp=\&arnumber=4483511\&isnumber=4731221$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4483511},
volume = {31},
year = {2009}
}
@article{Xiong2013,
abstract = {Many computer vision problems (e.g., camera calibra- tion, image alignment, structure from motion) are solved through a nonlinear optimization method. It is generally accepted that 2nd order descent methods are the most ro- bust, fast and reliable approaches for nonlinear optimiza- tion of a general smooth function. However, in the context of computer vision, 2nd order descent methods have two main drawbacks: (1) The function might not be analytically dif- ferentiable and numerical approximations are impractical. (2) The Hessian might be large and not positive definite. To address these issues, this paper proposes a Supervised Descent Method (SDM) for minimizing a Non-linear Least Squares (NLS) function. During training, the SDM learns a sequence of descent directions that minimizes the mean of NLS functions sampled at different points. In testing, SDM minimizes the NLS objective using the learned descent directions without computing the Jacobian nor the Hes- sian. We illustrate the benefits of our approach in synthetic and real examples, and show how SDM achieves state-of- the-art performance in the problem of facial feature detection. The code is available at www.humansensing.cs.cmu.edu/intraface.},
author = {Xiong, Xuehan and {De La Torre}, Fernando},
doi = {10.1109/CVPR.2013.75},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Xiong, De La Torre - 2013 - Supervised descent method and its applications to face alignment.pdf:pdf},
isbn = {1063-6919},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
keywords = {face alignment,facial feature tracking,non-linear least squares,supervised descent method},
pages = {532--539},
title = {{Supervised descent method and its applications to face alignment}},
year = {2013}
}
@article{Yang2002,
author = {Yang, Ming-Huang and Kriegman, David J and Ahuja, Narende},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Yang, Kriegman, Ahuja - 2002 - Detecting Faces In Image A Survey.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {1},
pages = {34--58},
title = {{Detecting Faces In Image : A Survey}},
volume = {24},
year = {2002}
}
@article{Yi2013,
abstract = {Most existing pose robust methods are too computational complex to meet practical applications and their perfor- mance under unconstrained environments are rarely evalu- ated. In this paper, we propose a novel method for pose ro- bust face recognition towards practical applications, which is fast, pose robust and can work well under unconstrained environments. Firstly, a 3D deformable model is built and a fast 3D model fitting algorithm is proposed to estimate the pose of face image. Secondly, a group of Gabor filters are transformed according to the pose and shape of face image for feature extraction. Finally, PCA is applied on the pose adaptive Gabor features to remove the redundances and Cosine metric is used to evaluate the similarity. The pro- posed method has three advantages: (1) The pose correc- tion is applied in the filter space rather than image space, which makes our method less affected by the precision of the 3D model; (2) By combining the holistic pose transforma- tion and local Gabor filtering, the final feature is robust to pose and other negative factors in face recognition; (3) The 3D structure and facial symmetry are successfully used to deal with self-occlusion. Extensive experiments on FERET and PIE show the proposed method outperforms state-of- the-art methods significantly, meanwhile, the method works well on LFW},
author = {Yi, Dong and Lei, Zhen and Li, Stan Z.},
doi = {10.1109/CVPR.2013.454},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Yi, Lei, Li - 2013 - Towards Pose Robust Face Recognition.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proc. IEEE Conf. on Computer Vision and Pattern Recognition},
pages = {3539--3545},
title = {{Towards Pose Robust Face Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619298},
year = {2013}
}
@article{Yi,
author = {Yi, Dong and Lei, Zhen and Liao, Shengcai and Li, Stan Z},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Yi et al. - Unknown - Learning Face Representation from Scratch.pdf:pdf},
title = {{Learning Face Representation from Scratch}}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Md and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1\_53},
eprint = {1311.2901},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Zeiler, Fergus - 2014 - Visualizing and understanding convolutional networks.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {Computer Vision–ECCV 2014},
pages = {818--833},
title = {{Visualizing and understanding convolutional networks}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-10590-1\_53},
volume = {8689},
year = {2014}
}
@article{Zhang2015,
author = {Zhang, Lei and Zhang, David},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Zhang, Zhang - 2015 - SVM and ELM Who Wins Object Recognition with Deep Convolutional Features from ImageNet.pdf:pdf},
keywords = {deep learning,extreme learning machine,image classification,machine,object recognition,support vector},
title = {{SVM and ELM: Who Wins? Object Recognition with Deep Convolutional Features from ImageNet}},
year = {2015}
}
@article{Zhou,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.04690v1},
author = {Zhou, Erjin},
eprint = {arXiv:1501.04690v1},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Zhou - Unknown - Naive-Deep Face Recognition Touching the Limit of LFW Benchmark or Not.pdf:pdf},
title = {{Naive-Deep Face Recognition : Touching the Limit of LFW Benchmark or Not ?}}
}
@article{Zhu2012,
author = {Zhu, X and Ramanan, D},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Zhu, Ramanan - 2012 - Face detection, pose estimation, and landmark estimation in the wild.pdf:pdf},
isbn = {9781467312288},
journal = {Proc. Int. Conf. on Computer Vision and Pattern Recognition (CVPR)},
title = {{Face detection, pose estimation, and landmark estimation in the wild.}},
year = {2012}
}
@article{Zhu2014,
archivePrefix = {arXiv},
arxivId = {1404.3543},
author = {Zhu, Zhenyao and Luo, P and Wang, Xiaogang and Tang, Xiaoou},
eprint = {1404.3543},
file = {:C$\backslash$:/Users/User/Documents/Mendeley Desktop/Zhu et al. - 2014 - Recover Canonical-View Faces in the Wild with Deep Neural Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1404.3543},
pages = {1--10},
title = {{Recover Canonical-View Faces in the Wild with Deep Neural Networks}},
url = {http://arxiv.org/abs/1404.3543},
year = {2014}
}
@article{jia2014caffe,
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
journal = {arXiv preprint arXiv:1408.5093},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
year = {2014}
}
@INPROCEEDINGS{1467360, 
	author={Dalal, N. and Triggs, B.}, 
	booktitle={Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on}, 
	title={Histograms of oriented gradients for human detection}, 
	year={2005}, 
	month={June}, 
	volume={1}, 
	pages={886-893 vol. 1}, 
	keywords={feature extraction;gradient methods;object detection;object recognition;support vector machines;coarse spatial binning;contrast normalization;edge based descriptors;fine orientation binning;fine-scale gradients;gradient based descriptors;histograms of oriented gradients;human detection;linear SVM;overlapping descriptor;pedestrian database;robust visual object recognition;High performance computing;Histograms;Humans;Image databases;Image edge detection;Object detection;Object recognition;Robustness;Support vector machines;Testing}, 
	doi={10.1109/CVPR.2005.177}, 
	ISSN={1063-6919},}
@article{Cortes:1995:SN:218919.218929,
	author = {Cortes, Corinna and Vapnik, Vladimir},
	title = {Support-Vector Networks},
	journal = {Mach. Learn.},
	issue_date = {Sept. 1995},
	volume = {20},
	number = {3},
	month = sep,
	year = {1995},
	issn = {0885-6125},
	pages = {273--297},
	numpages = {25},
	url = {http://dx.doi.org/10.1023/A:1022627411411},
	doi = {10.1023/A:1022627411411},
	acmid = {218929},
	publisher = {Kluwer Academic Publishers},
	address = {Hingham, MA, USA},
	keywords = {efficient learning algorithms, neural networks, pattern recognition, polynomial classifiers, radial basis function classifiers},
} 
@INPROCEEDINGS{4376991, 
	author={Smith, R.}, 
	booktitle={Document Analysis and Recognition, 2007. ICDAR 2007. Ninth International Conference on}, 
	title={An Overview of the Tesseract OCR Engine}, 
	year={2007}, 
	month={Sept}, 
	volume={2}, 
	pages={629-633}, 
	keywords={image classification;optical character recognition;Tesseract OCR engine;UNLV;adaptive classifier;line finding;Filters;Independent component analysis;Inspection;Open source software;Optical character recognition software;Pipelines;Prototypes;Search engines;Testing;Text recognition}, 
	doi={10.1109/ICDAR.2007.4376991}, 
	ISSN={1520-5363},}