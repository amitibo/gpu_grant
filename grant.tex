\documentclass{article} % For LaTeX2e
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{subcaption}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{Automatic Target Detection and Classification \\
  Deep Learning Course -  Final Project}


\author{
  Asst. Prof. Mark Silberstein \\
  mark@campus.technion.ac.il\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\newcommand{\cropsrow}[3]{\includegraphics[width=0.15\textwidth]{images/crops/#1.jpg} & #2 & \includegraphics[width=0.15\textwidth]{images/crops/mask_#1.jpg} & #3}

\begin{document}


\maketitle

\section{Summary}

In this project I demonstarte the use of Deeply-Learnt (DL)
Neural-Networks (NN) in a `Real-World' task. I.e. the NNs are trained to detect
and classify targets as part of the AUVSI Students Unmmanned Aerial Vehicle
competition.

\section{Introduction}

The Association for Unmanned Vehicle Systems International (AUVSI)
organizes the Student Unmanned Aerial System (SUAS) competition
\url{http://www.auvsi-seafarer.org/}. The competition is aimed  at
stimulating and fostering interest in unmanned system technologies.
The  student  are required  to  develop  and  provide  the  analysis,
design, fabrication and demonstration of a system capable of completing
specific autonomous aerial missions.

One of the missions is the automatic detection and classification of targets.
The targets are constructed of flat plywood of a given
size, basic geometric shape, and painted a color using flat paint.
The targets have different geometric shapes (triangles, circles, etc.)
and background colors; with a different color alphanumeric than the
background color.  The targets may be any mix of geometric shapes,
letters and colors. One of the targets is a 
QR Code target. Fig.~\ref{fig:targets} shows
some of the targets in this year's competition.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{auvsi_targets}
    \caption{Example of targets used in the AUVSI competition.}
    \label{fig:targets}
\end{figure}

The qualify for this mission, the system needs to identify three of the five
characteristics of the target: shape, color, character, character color and orientation.
In this project I develop and implemented a system of DL NN for the
task of automatically detecting and classifying the targets shape, character
and orientation.

\section{Background}

The task of this system is related to Region Convolutional Neural Networks
(R-CNN)~\cite{Girshick2014}. R-CNN systems revolutionized the area of object detection, classification, and segmentation. Fig.~\ref{fig:RCNN} show the diagram of a typical
R-CNN system. It is made of two stages, the first is a region proposal stage that extracts
object candidates. This regions are stretched into a predefined patch size and then
fed into a CNN. The CNN calculates confidence levels for a set of objects it was trained
for.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{RCNN}
	\caption{Diagram of a RCNN system}
	\label{fig:RCNN}
\end{figure}
 
\subsection{AUVSI project}

This is the second year that the Technion participates in the AUVSI SUAS
competition. Last year the team from CS developed a classical machine learning (ML)
system, similar to the one described in Sec.~\ref{sec:classical_mv_solution}.
Our team never managed to get the hands on all the source code of the system
neither did it work during the competition. Therefore I cannot evaluate it's
performance. But from this algorithm I borrow the region proposal algorithm:
Maximally Stable Extremal Regions (MSER)~\cite{Forssen2007}.

The student in charge of the automatic detection and classification this year
uses the MSER algorithm to find object candidates and then extracts
the contour of the most salient object in the crop. This contour is compared
to a library of shape contours. This is an interesting and promising approach.
But the student is yet to test his algorithm throughly neither did he deliver
his code. Given this situation I lack a previous system to compare this project to.

\subsection{Classical Machine Learning Approach}
\label{sec:classical_mv_solution}

Classical machine learning, traditionally approaches classification problems
by first transforming the data from the space of the problem to some feature
space. The data in this feature space is hopefully invariant to transforms common
in the feature space, e.g. similarity and affine transforms, occlusion, lighting
etc. It is assumed that in this feature space, objects of the same class reside
in some manifold and the manifolds of different objects are separated by regions
of low density.
Then some kind of a classifier, e.g. SVM is trained on this feature space to
discriminate between object of different classes.

This approach has been applied successfully to problem from many fields. But
finding the right
feature space for the problem and the right classifier for the feature space is
a complicated task that requires expertise.

I tried to implement a system using this approach. As feature space I
selected the Histogram Of Gradients (HOG)~\cite{1467360}, which was shown to
be very successful in detection tasks. I trained SVM~\cite{Cortes:1995:SN:218919.218929}
classifiers on synthetic data (described in Sec.\ref{sec:data_synthesis}). The
problem I tried to solve (distinguish between target and non-target objects) seemed
simpler than the problem in hand (classification to different shape classes). Nonetheless
the best SVM network I manged to train achieved $\le 85\%$ accuracy. A diagram
of this system is given in Fig.~\ref{fig:classical}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{classical_diagram}
	\caption{Diagram of a classical machine learning approach.}
	\label{fig:classical}
\end{figure}

I also tested the ability of a famous OCR code, tesseract-ocr~\cite{4376991}, to
classify the characters inside the targets. In my experiments the algorithm failed
to classify single letters. I assume that tesseract-ocr is trained to analyze complete
words and sentences and therefore is not fit for this task.

The conclusion of my experiments above is not that traditional ML systems
are useless. But that it requires high expertise in the field of the problem. This
project shows empirically that deeplearning network offer a good alternative to this
situation. 

\section{Proposed Research}

The system is comprised of two subsystems shown in \cref{fig:shape_diagram,fig:letter_diagram}.
The first subsystem detects and classifies target shapes in full (downscaled) image. The
input to this subsystem is a $1000\times750$px image. The original resolution of the
camera is $4000\times3000$px. The image is down-sampled for two reason, the first is that
the MSER algorithm is computationally demanding. The second is that the image processing is done
on the ground station. And the communication bandwidth between the drone and the ground station
is limited. The output is square crops classified as one of nine possible
shapes (circle, half circle, cross, rectangle, etc.). The crops are taken from full resolution
image and are fed into the
next subsystem that classifies the characters inside the shape. These subsystems
are described in the following subsections.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{diagram}
	\caption{Diagram of the shape classification subsystem}
	\label{fig:shape_diagram}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{letter_diagram}
	\caption{Diagram of the character classification subsystem}
	\label{fig:letter_diagram}
\end{figure}

\subsection{Shape Classification}
\label{sec:shape_class}

The first stage of shape classification is region proposal. I used the MSER feature
descriptor as blob detection. MSER detects regions (in gray scale images) that are
stable when different binarization threshold are applied to the image. In other
words MSER detects simply connected regions that look `differently' from their
surrounding (either darker or brighter). I always select a square patch centered
around the blob. This is in contrast to R-CNN where there region is selected as is
and then stretched into a square patch. The reason is that I don't want the geometric
shapes to be distorted, e.g. a circle to turn into an ellipse, a square into a rectangle
etc.

I decided to start with the CIFAR-10 CNN included in the Caffe. And in the end the
only change I did was to add a dropout layer before the fully connected layer 
(see \cref{fig:full_net}).
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{full_net}
	\caption{Diagram of the shape classification Caffe network}
	\label{fig:full_net}
\end{figure}
The motivation for using the CIFAR-10 network was that I had $\approx10$ geometric
shapes to classify and the basic network achieved high accuracy in training ($>99\%$).
The classes I trained the net to classify are: `circle', `half circle', `rectangle',
`triangle', `cross', `pentagon', `hexagon', `star', `qrcode', `no target'. Note the
last class, `no target'. It is needed as the region proposal algorithm has a very high
false positive rate (i.e. returns many patches with no target inside~\cref{fig:multiple_region_suggestions2})
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{multiple_region_suggestions2}
	\caption{Example of regions selected by the MSER algorithm}
	\label{fig:multiple_region_suggestions2}
\end{figure}
I trained the network on synthetic samples (see~\cref{fig:image_with_targets} and~\cref{sec:data_synthesis}) using three
constant learning rates (same as in the Caffe tutorial). The largest training set comprised of
$500K$ samples, half of them with target of random shape, color, character, orientation and scale.
The other half are patches with no target with them.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{image_with_targets}
	\caption{Synthetic targets (the only true target is the star at the top right with character `C')}
	\label{fig:image_with_targets}
\end{figure}

The network is trained for $\approx150$ epochs. Before adding the dropout layer the
network achieved $>99\%$ accuracy. I added a dropout layer before the fully connected layer to
avoid over-fitting.

Patches classified as `no-target' are dismissed. The rest of the patches are re-cropped from
the full resolution image and passed on to the character classification subsystem.

\subsection{Character Classification}
\label{sec:char_class}

The purpose of the character classification subsystem (\cref{fig:letter_diagram})  is to classify
the character inside the targets and its orientation. Its input are patches (taken from
the full resolution image) that were classified as containing target shapes. The first stage is
segmentation of the pixels belonging to the character. This is done using the `k-means' clustering
algorithm in the following manner:
\begin{enumerate}
	\item All the pixels in the patch are clustered according to their color into to groups: shape
	and background. The assumptions that allow this is that the background in the patch is relatively
	uniform. And that the number of pixels belonging to the character are relatively small.
	\item The pixels belonging to the cluster closer to the patch center are assumed to be the
	shape. A binary `shape' mask is formed by selecting these pixels.
	\item Any holes in the `shape' mask are filled (this is done in case the character pixels are
	colored clustered with the pixels belonging to the background).
	\item Pixels marked by the mask are clustered again by their color. The pixels belonging to tohe
	cluster closer to the patch center are assumed to belong to the character.
	\item A binary `character' mask is formed by selecting these pixels. See \cref{tab:patches} for 
	example of patches and their corresponding `character' mask.
\end{enumerate}

The `character' mask is rotated at a angles $0^\circ,15^\circ,30^\circ,\dots,345^\circ$ and re-cropped
tightly. This procedure forms $12$ copies of the binary mask at different angles (see \cref{fig:letter_diagram}).
These masks are re-sampled at $28\times28$px resolution and fed into a CNN trained for classifying
characters.

The CNN (\cref{fig:alpha_NN}) is based on the MNIST Caffe example, with a dropout layer.
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{alpha_train_test}
	\caption{Diagram of the character classification Caffe network}
	\label{fig:alpha_NN}
\end{figure}
It is trained to classify the input gray-scale patch into one of 28 classes: 26 uppercase characters,
`rotated' class, and `None' class. I added the `rotated' class because without it the
network would have a very high rate of false positives on rotated patches (i.e. it
would classify rotated characters wrongly as another character). The output of the network
is the probability score that the patch belongs to one of the 28 classes. If the class
with the highest probability is either `None' or `rotated', or that the probability of
the class is lower than some threshold, then the patch is dismissed as `no-target'.
I set the threshold empirically to 0.95.

As mentioned before the network is fed with 12 copies of the `character' mask rotated at
different angles. For each angle the network either discards the patch or selects for it
some character class.
The angle, whose character class receives the higest probability, is selected as the shape
orientation and the corresponding class is used as the shape character. If all the rotated
patches are discarded by the network, then the patch itself is discarded as `no-target'.
This way the character classification subsystem is used also as a weak target classifier. 

The network was trained with data from the following groups:
\begin{itemize}
	\item Straight (not rotated) characters. The character were drawn using random
	font, $\pm10^\circ$ orientation, scale and translation.
	\item Rotated characters. The same as above but the character was randomly rotated
	at the range $15^\circ$ to $345^\circ$.
	\item None. This class represents empty patches wrongly classified by the
	shape classification algorithm as targets. It is created by randomly cropping background
	patches and passing these through the `k-means' clustering algorithm described above.
\end{itemize}
The data comprised of $300$K samples equally divided between the above groups.

\section{Data Synthesis}
\label{sec:data_synthesis}

By far, the largest increases in the system performance were achieved by improving
the data synthesis process. The improvements involved understanding the type of
patches that the network `sees' in practice. Before this, the system would have
a very high rate of false positives (empty patches classified as valid shapes) and
false negatives (real targets classified as `no-target'). The first problem was
solved when I noticed that the region proposal algorithm select patches at 
multiple sizes, while the 'no-target' patches I synthesized where always of the
same size. Synthesizing patches of different sizes~\cref{fig:random_no_target}, drastically
reduced the rate of false positives.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.3\textwidth]{data_synthesis3}
	\caption{Example of random scale `no-target' patches.}
	\label{fig:random_no_target}
\end{figure}

The problem of false negatives was caused by the fact that the region proposal
algorithm does not crop `tightly' around the target shape~\cref{fig:sub1} while
the synthesized targets were drawn at the center of the patch~\cref{fig:sub2}
and tightly cropped. Loosening the `tight' cropping of the synthetic examples~\cref{fig:sub3}
decreased the number of false negative classifications.
Loosely placing 
\begin{figure}[h]
	\centering
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=.3\linewidth]{R}
		\caption{Proposed target region}
		\label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=.3\linewidth]{data_synthesis1}
		\caption{Tightly cropped sample}
		\label{fig:sub2}
	\end{subfigure}
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=.3\linewidth]{data_synthesis2}
		\caption{Loosely cropped sample}
		\label{fig:sub3}
	\end{subfigure}
	\caption{Imitating the way the region proposal algorithm crops around targets.}
	\label{fig:data_synthesis}
\end{figure}

The background of all synthetic data (targets and no-targets) were randomly cropped
from a subset of the $\approx1000$ images taken during the competition flight.
The background images were selected so that they will not include targets (that
might accidentally appear in `no-target' patches) on the one hand, and will include
as much clutter as possible (people, cars etc.), on the other hand.
It is important that the system used in next year's competition will be trained with
background images taken at multiple scenarios (different weather, time of day etc.). 

\section{Implementation}

The project was implemented using the Caffe~\cite{jia2014caffe} DL framework.
The Caffe framework was used through the python interface. This allowed for 
fast prototyping. Another benefit of using the Python interface was that I
was able to freely use it on the on EC2 amazon servers where I trained the Caffe
models.

The EC2 servers I used have a GRID K520 GPU. Training of a typical network ranges
from 2 to 6 hours depending on the data size. Classification of single image
takes approximately 600ms.

\section{Results}
\label{sec:results}

The system was validated on $\approx400$ images taken during the target detection mission
of the competition (the rest of the $>600$ images not used for validation were taken
during other missions). The small number of targets that appear in this images (four shape
and one QR targets) means that the validation is only qualitative and doesn't have
real quantitative value. Nonetheless this is the best I could do to imitate a real
competition scenario, and serves for comparing different networks performance.

In the following figures, regions classified as non-target by the shape classifier are
marked by red square. Regions classified as valid shapes by the shape classifier but
as no-target by the character classifier are marked by yellow square. Regions classified
correctly by both subsystems are marked by blue square. \Cref{fig:true_positive1,fig:true_positive2} show two examples of targets classified
correctly both by the shape subsystem and the letter subsystem. 
\Cref{fig:false_positive1,fig:false_positive2} show two examples of non-target regions
(marked by yellow square) misclassified by the shape classifier but correctly classified
by the character classifier (the region classified as `qrcode' in~\cref{fig:false_positive2}
is an exception as it doesn't include a character and therefore its character classification
as no-target is invalid). \Cref{fig:misclass} is an example a target classified as the wrong
shape (triangle instead of half-circle). This is the only shape misclassification in the
validation set. \Cref{fig:false_negative1} is an example of a target misclassified as no-target
by the shape classifier. This is the only target in the validation set that the system missed.

\Cref{tab:patches} shows examples of pathces classified as shapes (both true and false positives),
their `character' mask, and character classification.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{true_positive_correct_letter1}
	\caption{Target classified correctly as: shape=rectangle, char=U and angle=$30^\circ$.}
	\label{fig:true_positive1}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{true_positive_correct_letter2}
	\caption{Target classified correctly as: shape=star, char=C and angle=$105^\circ$.}
	\label{fig:true_positive2}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{false_postive_weak_classification1}
	\caption{Example of false positive of the shape classification but correct character classification.}
	\label{fig:false_positive1}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{false_postive_weak_classification2}
	\caption{Example of false positive of the shape classification but correct character classification.}
	\label{fig:false_positive2}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{misclassfication}
	\caption{Shape misclassification: Half circle target misclassified as triangle.}
	\label{fig:misclass}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{false_negative1}
	\caption{False negative: the shape classifier misclassified the star target as `no-target'.}
	\label{fig:false_negative1}
\end{figure}

In an effort to improve performance I tried to replace the single shape network by
an ensemble of networks.
I trained three shape networks (of the same model) on three different sets of data. Each
one of the data sets included $150$K samples. But in the validation the use of an ensemble of
networks increased the number of false negatives (missed targets). Nonetheless I think that
ensemble of networks is a worthwhile direction and increasing the number of networks
might improve its performance.

\begin{table}[ht]
\centering
\begin{tabular}{llll}
\multicolumn{1}{c}{\bf crop}  & \multicolumn{1}{c}{\bf shape class.} &\multicolumn{1}{c}{\bf `character' mask} &\multicolumn{1}{c}{\bf letter class.} \\
\hline
\cropsrow{1}{`CROSS'}{NO TARGET} \\
\hline
\cropsrow{2}{`RECTANGLE'}{`U' at $30^{\circ}$} \\
\hline
\cropsrow{5}{`CIRCLE'}{`R' at $315^{\circ}$} \\
\hline
\cropsrow{7}{`QR code'}{no target} \\
\hline
\cropsrow{16}{'QR code'}{no target} \\
\hline
\cropsrow{17}{CROSS}{no target} \\
\hline
\cropsrow{27}{`STAR'}{`C' at $105^{\circ}$} \\
\hline
\end{tabular}
\caption{Crops classified as valid targets and their Alphanumeric classification.}
\label{tab:patches}
\end{table}

\section{Conclusions and Future Work}

This project demonstrates the capability of deep-learning networks to solve
`real world' problems. It also shows that it is possible to train a network on synthetic
data. But it is important that the data will fit the classification process.

We are already in the process of recruiting the students for next year's
competition. The results of this project offer a starting point
for exploring new ideas:
\begin{itemize}
\item Try ensemble of small DL networks.
\item Explore other network architectures.
\item Explore other region proposal algorithms (e.g. based on color statistics).
\item Train the network on different scenarios (different wheather, time of day etc.).
\end{itemize}

Recently NVIDIA released mini boards with strong GPU's \url{https://developer.nvidia.com/jetson-tk1}. It can be interesting to try
to implement a similar system on a board like this. This board can serve as the computer
on-board the drone. Thus the image processing tasks can be performed `in the air' and not in the ground station as it is done today.

\section{Budget Plan}

\begin{center}
	\begin{tabular}{ | l | l | l | }
		\hline
				& Item 											& Budget (NIS) 	\\ \hline
		1		& MSc Student 									& 20,000    	\\ \hline
		2		& 1 global shutter machine vision camera 		& 20,000 		\\ \hline
		3 		& 1 computer and accessories 					& 10,000 		\\ \hline
		4 		& 4 GPU Nvidia Jetson TX1	 					& 12,000 		\\ \hline
		5 		& Operating expenses (Flight tests) 			& 10,000 		\\ \hline
		6 		& Supplies and publication	 					& 8,000 		\\ \hline
		7 		& Transportation and communication				& 5,000 		\\ \hline
		8 		& Students travel 								& 10,000 		\\ \hline
		9 		& Laboratory staff (engineers \& technicians)	& 20,000 		\\ \hline
		Total 	&  							 					& 115,000 		\\ \hline
	\end{tabular}
\end{center}

\section{Mark Siberstein - Short Biography}

\bibliography{project}
\bibliographystyle{unsrt}
\end{document}



